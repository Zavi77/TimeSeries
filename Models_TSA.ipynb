{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.49767289]\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from random import random\n",
    "# contrived dataset\n",
    "data = [x + random() for x in range(1, 100)]\n",
    "# fit model\n",
    "model = ARIMA(data, order=(1, 1, 1))\n",
    "model_fit = model.fit()\n",
    "# make prediction\n",
    "yhat = model_fit.predict(len(data), len(data), typ='levels')\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\Download\\Programs\\DwnlData\\avijeet\\Anaconda\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "A:\\Download\\Programs\\DwnlData\\avijeet\\Anaconda\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:949: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.22074146]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\Download\\Programs\\DwnlData\\avijeet\\Anaconda\\lib\\site-packages\\statsmodels\\base\\model.py:512: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SARIMA example\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from random import random\n",
    "# contrived dataset\n",
    "data = [x + random() for x in range(1, 100)]\n",
    "# fit model\n",
    "model = SARIMAX(data, order=(1, 1, 1), seasonal_order=(0, 0, 0, 0))\n",
    "model_fit = model.fit(disp=False)\n",
    "# make prediction\n",
    "yhat = model_fit.predict(len(data), len(data))\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMAX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.31037611]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SARIMAX example\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from random import random\n",
    "# contrived dataset\n",
    "data1 = [x + random() for x in range(1, 100)]\n",
    "data2 = [x + random() for x in range(101, 200)]\n",
    "# fit model\n",
    "model = SARIMAX(data1, exog=data2, order=(1, 1, 1), seasonal_order=(0, 0, 0, 0))\n",
    "model_fit = model.fit(disp=False)\n",
    "# make prediction\n",
    "exog2 = [200 + random()]\n",
    "yhat = model_fit.predict(len(data1), len(data1), exog=[exog2])\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.23865558 100.79153004]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\Download\\Programs\\DwnlData\\avijeet\\Anaconda\\lib\\site-packages\\statsmodels\\base\\wrapper.py:35: FutureWarning: y is a deprecated alias for endog, will be removed in version 0.11.0\n",
      "  obj = getattr(results, attr)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# VAR example\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from random import random\n",
    "# contrived dataset with dependency\n",
    "data = list()\n",
    "for i in range(100):\n",
    "    v1 = i + random()\n",
    "    v2 = v1 + random()\n",
    "    row = [v1, v2]\n",
    "    data.append(row)\n",
    "# fit model\n",
    "model = VAR(data)\n",
    "model_fit = model.fit()\n",
    "# make prediction\n",
    "yhat = model_fit.forecast(model_fit.y, steps=1)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARMAX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\Download\\Programs\\DwnlData\\avijeet\\Anaconda\\lib\\site-packages\\statsmodels\\tsa\\statespace\\varmax.py:159: EstimationWarning: Estimation of VARMA(p,q) models is not generically robust, due especially to identification issues.\n",
      "  EstimationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44479352 0.92444803]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# VARMAX example\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from random import random\n",
    "# contrived dataset with dependency\n",
    "data = list()\n",
    "for i in range(100):\n",
    "    v1 = random()\n",
    "    v2 = v1 + random()\n",
    "    row = [v1, v2]\n",
    "    data.append(row)\n",
    "data_exog = [x + random() for x in range(100)]\n",
    "# fit model\n",
    "model = VARMAX(data, exog=data_exog, order=(1, 1))\n",
    "model_fit = model.fit(disp=False)\n",
    "# make prediction\n",
    "data_exog2 = [[100]]\n",
    "yhat = model_fit.forecast(exog=data_exog2)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HWES model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99.01425312]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# HWES example\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from random import random\n",
    "# contrived dataset\n",
    "data = [x + random() for x in range(1, 100)]\n",
    "# fit model\n",
    "model = ExponentialSmoothing(data)\n",
    "model_fit = model.fit()\n",
    "# make prediction\n",
    "yhat = model_fit.predict(len(data), len(data))\n",
    "print(yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
